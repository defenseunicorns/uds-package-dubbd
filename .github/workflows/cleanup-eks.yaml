name: Cleanup EKS

on:
  push:
#  workflow_dispatch:
#    inputs:
#     eks_cluster_name:
#       required: true
#       type: string

env:
  eks_cluster_name: uds-core-aws-1beb169


permissions:
  id-token: write
  contents: read

jobs:
  delete-eks:
    runs-on: ubuntu-latest
  
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_COMMERCIAL_ROLE_TO_ASSUME }}
          role-session-name: ${{ github.job || github.event.client_payload.pull_request.head.sha || github.sha }}
          aws-region: us-west-2
          role-duration-seconds: 21600

      - name: Get cluster kubeconfig
        id: get_cluster_kubeconfig
        run: |
          eksctl utils write-kubeconfig -c ${eks_cluster_name}
#        with: 
#          eks_cluster_name: "${{inputs.eks_cluster_name}}"
          
      - name: Show Cluster
        run: |
          kubectl get nodes
          kubectl config get-contexts

#      - name: Teardown EKS cluster
#        if: always()
#        # can't do a zarf package remove since there's no kubernetes cluster.
#        run: |
#          cat config.yaml | sed "s/###ZARF_VAR_CLUSTER_NAME###/${eks_cluster_name}/g" | eksctl delete cluster -f - --disable-nodegroup-eviction --wait
#        with:
#          eks_cluster_name: "${{inputs.eks_cluster_name}}"
#        working-directory: .github/workflows/eks
#        timeout-minutes: 60
#        continue-on-error: true